{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Homework 3**\n",
        "## Install Necessary Tools and libraries"
      ],
      "metadata": {
        "id": "piYGI_h03x_7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XtS_1U7uWoSz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b7aac8b-7400-4387-b056-504781a064af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.2/216.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.5/327.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qqq tiktoken\n",
        "!pip install -qqq pinecone-client\n",
        "!pip install -qqq pypdf\n",
        "!pip install -qqq openai\n",
        "!pip install -qqq groq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-openai"
      ],
      "metadata": {
        "id": "CwMoVpfTX4e_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a74bf624-852f-4677-db15-33ca72fe39ca"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m332.8/332.8 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.4/127.4 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the api key from OpenAI, Pinecone, & Groq"
      ],
      "metadata": {
        "id": "SFPki-373toV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from groq import Groq\n",
        "OPENAI_API_KEY=userdata.get('openai-key')\n",
        "PINECONE_API_KEY=userdata.get('pinecone-key')\n",
        "GROQ_API_KEY=userdata.get('groq-key')\n",
        "!pip install -qqq langchain_community\n",
        "from langchain_community.document_loaders.pdf import PyPDFDirectoryLoader"
      ],
      "metadata": {
        "id": "O3IVu_sJXhxL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b82981bf-5be7-4f7d-8eb9-3daeceb4dbb1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m975.5/975.5 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
        "os.environ['PINECONE_API_KEY'] = PINECONE_API_KEY\n",
        "os.environ['GROQ_API_KEY'] = GROQ_API_KEY"
      ],
      "metadata": {
        "id": "H5P1KMv2noyB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the document"
      ],
      "metadata": {
        "id": "ui4OpszP4QPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2\n",
        "from PyPDF2 import PdfReader\n",
        "import os\n",
        "\n",
        "def read_doc(directory: str) -> list[str]:\n",
        "    # Function to read the PDFs from a directory.\n",
        "\n",
        "    # Args:\n",
        "    # directory (str): The path of the directory where the PDFs are stored.\n",
        "\n",
        "    # Returns:\n",
        "    # list[str]: A list of text in the PDFs.\n",
        "\n",
        "    page_contents = []\n",
        "\n",
        "    # List all PDF files in the directory\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith('.pdf'):\n",
        "            filepath = os.path.join(directory, filename)\n",
        "\n",
        "            # Open the PDF file\n",
        "            reader = PdfReader(filepath)\n",
        "            num_pages = len(reader.pages)\n",
        "\n",
        "            # Extract text from each page\n",
        "            for page_num in range(num_pages):\n",
        "                page = reader.pages[page_num]\n",
        "                page_contents.append(page.extract_text())\n",
        "\n",
        "    return page_contents\n",
        "\n",
        "# Call the function\n",
        "full_document = read_doc(\"/content\")\n",
        "print(len(full_document))  # This should print the number of pages read\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnZ7Xg3nXjFB",
        "outputId": "5c13624a-4951-4004-9c84-eb212f995f3c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m225.3/232.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Embeddings for only 15 pages\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "R56kRk6J1grp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq langchain_openai\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# Initialize the embedding model\n",
        "embed_model = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
        "\n",
        "def generate_embeddings(documents: list[str]) -> list[list[float]]:\n",
        "    # Generate embeddings for a list of documents.\n",
        "    # Args:\n",
        "    # documents (list[str]): A list of document objects, each containing a 'page_content' attribute.\n",
        "    # Returns:\n",
        "    # list[list[float]]: A list containing a list of embeddings corresponding to the documents.\n",
        "\n",
        "    embedded = [embed_model.embed_documents([doc]) for doc in documents]\n",
        "    return embedded\n",
        "\n",
        "# Extract the first 15 pages from full_document\n",
        "selected_documents = full_document[:15]\n",
        "\n",
        "# Run the function\n",
        "chunked_document_embeddings = generate_embeddings(documents=selected_documents)\n",
        "\n",
        "# Check the dimension of embedding model\n",
        "print(len(chunked_document_embeddings))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SYNUj1lr1e_p",
        "outputId": "a194f9eb-f97b-4fde-d68e-1cd63778a5ce"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create unique IDs\n",
        "ids = [str(x) for x in range(0,len(chunked_document_embeddings))]\n",
        "\n",
        "def combine_vector_and_text(\n",
        "    documents: list[any], doc_embeddings: list[list[float]]) -> list[dict[str, any]]:\n",
        "    \"\"\"\n",
        "    Process a list of documents along with their embeddings.\n",
        "\n",
        "    Args:\n",
        "    - documents (List[Any]): A list of documents (strings or other types).\n",
        "    - doc_embeddings (List[List[float]]): A list of embeddings corresponding to the documents.\n",
        "\n",
        "    Returns:\n",
        "    - data_with_metadata (List[Dict[str, Any]]): A list of dictionaries, each containing an ID, embedding values, and metadata.\n",
        "    \"\"\"\n",
        "    data_with_metadata = []\n",
        "\n",
        "    for id,doc_text, embedding in zip(ids,documents, doc_embeddings):\n",
        "        # Convert doc_text to string if it's not already a string\n",
        "        if not isinstance(doc_text, str):\n",
        "            doc_text = str(doc_text)\n",
        "\n",
        "        # Generate a unique ID based on the text content\n",
        "        doc_id = id\n",
        "\n",
        "        # Create a data item dictionary\n",
        "        data_item = {\n",
        "            \"id\": doc_id,\n",
        "            \"values\": embedding[0],\n",
        "            \"metadata\": {\"text\": doc_text},  # Include the text as metadata\n",
        "        }\n",
        "\n",
        "        # Append the data item to the list\n",
        "        data_with_metadata.append(data_item)\n",
        "\n",
        "    return data_with_metadata\n",
        "\n",
        "\n",
        "# Call the function\n",
        "all_meta_data = combine_vector_and_text(full_document, chunked_document_embeddings)"
      ],
      "metadata": {
        "id": "AoUAe4or9K5f"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up the Pinecone Indexes"
      ],
      "metadata": {
        "id": "xYpF6Ncy6Ol_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import Pinecone,ServerlessSpec\n",
        "pinecone = Pinecone()\n",
        "\n",
        "INDEX_NAME=\"rag-pdf\"\n",
        "if INDEX_NAME in [index.name for index in pinecone.list_indexes()]:\n",
        "  pinecone.delete_index(INDEX_NAME)\n",
        "\n",
        "pinecone.create_index(name=INDEX_NAME, dimension=3072, metric='cosine',\n",
        "  spec=ServerlessSpec(cloud='aws', region='us-east-1'))\n",
        "\n",
        "index = pinecone.Index(INDEX_NAME)"
      ],
      "metadata": {
        "id": "Pu2l9lz8_De6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cC5dXCyqCNXR",
        "outputId": "f1bdc57c-bfe7-468b-ae7a-cf62e334e3c6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pinecone.data.index.Index object at 0x7c4c52f78a00>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index.upsert(all_meta_data)"
      ],
      "metadata": {
        "id": "Avxgo8LjCkWa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "005e8774-99da-418a-ae7c-9f43698720b4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'upserted_count': 15}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, query the pincone indexes"
      ],
      "metadata": {
        "id": "NBstvng86fGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "# Generate embeddings for the query\n",
        "def query_pinecone_index(query_embeddings: list, top_k=3, include_metadata: bool = True) -> dict:\n",
        "    \"\"\"\n",
        "    Query a Pinecone index.\n",
        "\n",
        "    Args:\n",
        "    - query_embeddings (list): List of query embeddings.\n",
        "    - top_k (int): Number of nearest neighbors to retrieve (default: 3).\n",
        "    - include_metadata (bool): Whether to include metadata in the query response (default: True).\n",
        "\n",
        "    Returns:\n",
        "    - query_response (dict): Query response containing nearest neighbors.\n",
        "    \"\"\"\n",
        "    query_response = index.query(\n",
        "        vector=query_embeddings, top_k=top_k, include_metadata=include_metadata\n",
        "    )\n",
        "    return query_response"
      ],
      "metadata": {
        "id": "ZWZBJ6UMD3Eq"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Red Teaming with Thresholds\n",
        "To compare the similarity of the query in the context of provided document, I have set a threshold for the similarity score. The similarity score is returned as a result of querying the Pinecone indexes after receiving a query. The threshold was determined after observing the score values from several queries that were both in-context and out-of-context.\n",
        "\n",
        "If the similarity score exceeds the threshold, the application identifies the query as valid. Then, it will look for the most similar chunks within the document.\n",
        "\n",
        "Implementing this technique, will help ward off certain kinds of off-topic queries that have no relation to the document provided.\n"
      ],
      "metadata": {
        "id": "WwQo4N5FOw-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def query_with_similarity_threshold(query: str, threshold: float = 0.4) -> str:\n",
        "    \"\"\"\n",
        "    Handle the query by determining if it is on-topic or off-topic using red teaming.\n",
        "\n",
        "    Args:\n",
        "    - query (str): User's query.\n",
        "    - threshold (float): Similarity threshold to classify questions as on-topic or off-topic.\n",
        "\n",
        "    Returns:\n",
        "    - str: Response to the query.\n",
        "    \"\"\"\n",
        "    # Generate embedding for the query\n",
        "    query_embedding = embed_model.embed_query(query)\n",
        "\n",
        "    # Query the Pinecone index\n",
        "    query_response = query_pinecone_index(query_embeddings=query_embedding)\n",
        "\n",
        "    # Extract the similarities and metadata\n",
        "    similarities = [result['score'] for result in query_response['matches']]\n",
        "    documents = [result['metadata']['text'] for result in query_response['matches']]\n",
        "\n",
        "    # Check if the query is on-topic\n",
        "    max_similarity = max(similarities)\n",
        "    if max_similarity < threshold:\n",
        "        return \"I'm sorry, the query is out of context in relation to the document provide!\"\n",
        "\n",
        "    # Find the most relevant document chunk\n",
        "    most_relevant_index = np.argmax(similarities)\n",
        "    return f\"{documents[most_relevant_index]}\"\n"
      ],
      "metadata": {
        "id": "tWyCN2rXpW8U"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, invoke the LLM to provide the response after determining that the query is valid query."
      ],
      "metadata": {
        "id": "7KRiEKlV8GBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read Teaming Validation\n",
        "Ouery 1 - Out of context"
      ],
      "metadata": {
        "id": "f46iIm7sh4ow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "query = \"Is Taylor Swift better than Katy Perry?\"\n",
        "response = query_with_similarity_threshold(query)\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "LLM1 = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n",
        "LLM2 = ChatOpenAI(temperature=0, model_name=\"gpt-4\")\n",
        "LLM3 = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")\n",
        "\n",
        "# Extract the text from the dictionary before passing it to the LLM\n",
        "prompt = f\"{response} Using only the provided information, give me a summarized answer on {query}\"\n",
        "\n",
        "def query_response(prompt: str, LLM) -> str:\n",
        "    \"\"\"This function returns a better response using LLM\n",
        "    Args:\n",
        "        prompt (str): The prompt template\n",
        "\n",
        "    Returns:\n",
        "        str: The actual response returned by the LLM\n",
        "    \"\"\"\n",
        "    answer = LLM.invoke(prompt)\n",
        "    return answer.content\n",
        "\n",
        "# Response from the red teaming technique\n",
        "print(f\"\\n RAG answer: \\n {response}\")\n",
        "\n",
        "# Call three LLM functions\n",
        "final_answer = query_response(prompt=prompt, LLM=LLM1)\n",
        "print(f\"\\n Response of gpt-3.5-turbo: \\n {final_answer}\")\n",
        "\n",
        "final_answer = query_response(prompt=prompt, LLM=LLM2)\n",
        "print(f\"\\n Response of GPT-4: \\n {final_answer}\")\n",
        "\n",
        "final_answer = query_response(prompt=prompt, LLM=LLM3)\n",
        "print(f\"\\n Response of GPT-4omni: \\n {final_answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-7nFchZENQ2",
        "outputId": "6c935efc-5761-4545-83f7-90716aaf56a5"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " RAG answer: \n",
            " I'm sorry, the query is out of context in relation to the document provide!\n",
            "\n",
            " Response of gpt-3.5-turbo: \n",
            " It is subjective and ultimately up to personal preference as to whether Taylor Swift is better than Katy Perry. Both artists have achieved great success in the music industry and have their own unique styles and fan bases.\n",
            "\n",
            " Response of GPT-4: \n",
            " The document does not provide information on whether Taylor Swift is better than Katy Perry.\n",
            "\n",
            " Response of GPT-4omni: \n",
            " The provided information does not include any specific details or criteria to compare Taylor Swift and Katy Perry. Therefore, I cannot provide a summarized answer on whether Taylor Swift is better than Katy Perry based on the given context.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "In the first query, I provided a totally different question that isn't related to the document. The response after the RAG search was as expected. The similarity score didn't pass the threshold and provided a fall-back response, which was expected.\n",
        "\n",
        "In case of 3 different GPT models, the results were mixed. 3.5-turbo didn't seem to be able to handle off-topic queries as good as GPT-4 and GPT-4o. It still relied on its training data, completely ignoring the prompt which explicitly says, \"using only the provided information\"."
      ],
      "metadata": {
        "id": "8ncGMygl-4l7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Query2"
      ],
      "metadata": {
        "id": "2imEQy8LiC9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "query = \"Is General Motors building new Cadillac BLS at Saab factory in Sweden?\"\n",
        "response = query_with_similarity_threshold(query)\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "LLM1 = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n",
        "LLM2 = ChatOpenAI(temperature=0, model_name=\"gpt-4\")\n",
        "LLM3 = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")\n",
        "\n",
        "# Extract the text from the dictionary before passing it to the LLM\n",
        "prompt = f\"{response} Using only the provided information, give me a summarized answer on {query}\"\n",
        "\n",
        "def query_response(prompt: str, LLM) -> str:\n",
        "    \"\"\"This function returns a better response using LLM\n",
        "    Args:\n",
        "        prompt (str): The prompt template\n",
        "\n",
        "    Returns:\n",
        "        str: The actual response returned by the LLM\n",
        "    \"\"\"\n",
        "    answer = LLM.invoke(prompt)\n",
        "    return answer.content\n",
        "\n",
        "# Response from the red teaming technique\n",
        "print(f\"\\n RAG answer: \\n {response}\")\n",
        "\n",
        "# Call three LLM functions\n",
        "final_answer = query_response(prompt=prompt, LLM=LLM1)\n",
        "print(f\"\\n Response of gpt-3.5-turbo: \\n {final_answer}\")\n",
        "\n",
        "final_answer = query_response(prompt=prompt, LLM=LLM2)\n",
        "print(f\"\\n Response of GPT-4: \\n {final_answer}\")\n",
        "\n",
        "final_answer = query_response(prompt=prompt, LLM=LLM3)\n",
        "print(f\"\\n Response of GPT-4omni: \\n {final_answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYLrfM05h-Lr",
        "outputId": "93f4ed4f-8a65-4aea-b123-3d3cbe70533c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " RAG answer: \n",
            " Most areas of the US saw their economy continue to expand in December and \n",
            "early January, the US Federal Reserve said in its latest Beige Book report. \n",
            " Of the 12 US regions it identifies for the study, 11 showed stronger \n",
            "economic growth, with only the Cleveland area falling behind with a \n",
            "\"mixed\" rating. Consumer spending was higher in December than November, \n",
            "and festive sales were also up on 2003. The employment picture also \n",
            "improved, the Fed said.  \"Labour markets firmed in a number of districts, but wage pressures generally remained modest,\" the Beige Book said. \"Several districts reported higher prices for building materials and manufacturing inputs, \n",
            "but most reported steady or only slightly higher overall price levels.\" \n",
            "The report added that residential real estate activity remained strong \n",
            "and that commercial real estate activity strengthened in most districts. \n",
            "\"Office leasing was especially brisk in Washington DC, and New York City, \n",
            "two of the nation's strongest commercial markets,\" the Fed said.  \n",
            " \n",
            "-------------------------------------------------------------------------------- \n",
            "-------------------------------------------------------------------------------- \n",
            "Saab to build Cadillacs in Sweden  \n",
            " General Motors, the world's largest car maker, has confirmed that it will \n",
            "build a new medium -sized Cadillac BLS at its loss -making Saab factory in \n",
            "Sweden. \n",
            " \n",
            "The car, unveiled at the Geneva motor show, is intended to compete in the \n",
            "medium-sized luxury car market. It will not be sold in the US, said GM \n",
            "Europe president Carl- Peter Forster. As part of its efforts to make the \n",
            "US marque appeal to European drivers, the car will be the first Cadillac with a diesel engine.  \n",
            " GM's announcement should go some way to allay fears of the Saab factory's closure. The factory in Trollhaettan has been at the centre of rumours about GM's planned severe cutbacks in its troubled European operations. \n",
            "But the group's new commitment to the Swedish factory may not be welcomed \n",
            "by the group's Opel workers in Ruesselsheim, Germany. They may now have to face a larger proportion of GM's cuts.  \n",
            " Neither will the announcement be seen as unalloyed good news in Sweden, since it reflects Saab's failure to make significant inroads into the \n",
            "lucrative European luxury car market. For years, Saab has consistently \n",
            "said it is competing head- on with BMW, Mercedes and Jaguar. The segment's \n",
            "leaders do not agree.  \n",
            " GM's plans to build the American marque in Sweden is part of its efforts to push it as an alternative luxury brand for European drivers. In the US, it has long been established as an upmarket brand -  even the \n",
            "presidential limousine carries the badge. Yet it could prove tough for Cadillac to steal market share from the majors in Europe. Other luxury car makers, most notably the Toyota subsidiary Lexus, have enjoyed \n",
            "\n",
            " Response of gpt-3.5-turbo: \n",
            " Yes, General Motors has confirmed that it will build a new medium-sized Cadillac BLS at its Saab factory in Sweden. The car is intended to compete in the medium-sized luxury car market in Europe and will be the first Cadillac to have a diesel engine. This decision is part of GM's efforts to make the Cadillac brand more appealing to European drivers.\n",
            "\n",
            " Response of GPT-4: \n",
            " Yes, General Motors has confirmed that it will build a new medium-sized Cadillac BLS at its Saab factory in Sweden.\n",
            "\n",
            " Response of GPT-4omni: \n",
            " Yes, General Motors is building the new Cadillac BLS at its Saab factory in Sweden. This decision is part of GM's strategy to make the Cadillac brand appeal to European drivers, and the Cadillac BLS will be the first Cadillac with a diesel engine. The move aims to address concerns about the potential closure of the Saab factory in Trollhaettan, although it may lead to more significant cutbacks at GM's Opel operations in Germany.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "\n",
        "When asking questions about the information in the document, the results aren't that surprising. The response from RAG provides a segment of the news about GM building new Cadillacs in Sweden factory. This is exactly what I was looking for.\n",
        "\n",
        "The LLMs' response was also straighforward, all three, provided similar response agreeing to the fact that General Motors has confirmed that it will start building a new Cadillac BLS at its Sweden factory. 3.5-turbo and GPT-4o provided a more elaborate response with some details, whereas, GPT-4's response was more concise compared to the other 2 models."
      ],
      "metadata": {
        "id": "p98472ozFjPr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Query3"
      ],
      "metadata": {
        "id": "PPeMh-zSiFnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Tell me about Cadillac and Sweden.\"\n",
        "# query = \"How is the Japanese industry reviving back from recession? \"\n",
        "response = query_with_similarity_threshold(query)\n",
        "\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "LLM1 = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n",
        "LLM2 = ChatOpenAI(temperature=0, model_name=\"gpt-4\")\n",
        "LLM3 = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")\n",
        "\n",
        "# Extract the text from the dictionary before passing it to the LLM\n",
        "prompt = f\"{response} Using only the provided information, give me a summarized answer on {query}\"\n",
        "\n",
        "def query_response(prompt: str, LLM) -> str:\n",
        "    \"\"\"This function returns a better response using LLM\n",
        "    Args:\n",
        "        prompt (str): The prompt template\n",
        "\n",
        "    Returns:\n",
        "        str: The actual response returned by the LLM\n",
        "    \"\"\"\n",
        "    answer = LLM.invoke(prompt)\n",
        "    return answer.content\n",
        "\n",
        "# Response from the red teaming technique\n",
        "print(f\"\\n RAG answer: \\n {response}\")\n",
        "\n",
        "# Call three LLM functions\n",
        "final_answer = query_response(prompt=prompt, LLM=LLM1)\n",
        "print(f\"\\n Response of gpt-3.5-turbo: \\n {final_answer}\")\n",
        "\n",
        "final_answer = query_response(prompt=prompt, LLM=LLM2)\n",
        "print(f\"\\n Response of GPT-4: \\n {final_answer}\")\n",
        "\n",
        "final_answer = query_response(prompt=prompt, LLM=LLM3)\n",
        "print(f\"\\n Response of GPT-4omni: \\n {final_answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWszjFBxh-g8",
        "outputId": "ae063892-7c80-46c7-cfe6-717bad1a6b0b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " RAG answer: \n",
            " Most areas of the US saw their economy continue to expand in December and \n",
            "early January, the US Federal Reserve said in its latest Beige Book report. \n",
            " Of the 12 US regions it identifies for the study, 11 showed stronger \n",
            "economic growth, with only the Cleveland area falling behind with a \n",
            "\"mixed\" rating. Consumer spending was higher in December than November, \n",
            "and festive sales were also up on 2003. The employment picture also \n",
            "improved, the Fed said.  \"Labour markets firmed in a number of districts, but wage pressures generally remained modest,\" the Beige Book said. \"Several districts reported higher prices for building materials and manufacturing inputs, \n",
            "but most reported steady or only slightly higher overall price levels.\" \n",
            "The report added that residential real estate activity remained strong \n",
            "and that commercial real estate activity strengthened in most districts. \n",
            "\"Office leasing was especially brisk in Washington DC, and New York City, \n",
            "two of the nation's strongest commercial markets,\" the Fed said.  \n",
            " \n",
            "-------------------------------------------------------------------------------- \n",
            "-------------------------------------------------------------------------------- \n",
            "Saab to build Cadillacs in Sweden  \n",
            " General Motors, the world's largest car maker, has confirmed that it will \n",
            "build a new medium -sized Cadillac BLS at its loss -making Saab factory in \n",
            "Sweden. \n",
            " \n",
            "The car, unveiled at the Geneva motor show, is intended to compete in the \n",
            "medium-sized luxury car market. It will not be sold in the US, said GM \n",
            "Europe president Carl- Peter Forster. As part of its efforts to make the \n",
            "US marque appeal to European drivers, the car will be the first Cadillac with a diesel engine.  \n",
            " GM's announcement should go some way to allay fears of the Saab factory's closure. The factory in Trollhaettan has been at the centre of rumours about GM's planned severe cutbacks in its troubled European operations. \n",
            "But the group's new commitment to the Swedish factory may not be welcomed \n",
            "by the group's Opel workers in Ruesselsheim, Germany. They may now have to face a larger proportion of GM's cuts.  \n",
            " Neither will the announcement be seen as unalloyed good news in Sweden, since it reflects Saab's failure to make significant inroads into the \n",
            "lucrative European luxury car market. For years, Saab has consistently \n",
            "said it is competing head- on with BMW, Mercedes and Jaguar. The segment's \n",
            "leaders do not agree.  \n",
            " GM's plans to build the American marque in Sweden is part of its efforts to push it as an alternative luxury brand for European drivers. In the US, it has long been established as an upmarket brand -  even the \n",
            "presidential limousine carries the badge. Yet it could prove tough for Cadillac to steal market share from the majors in Europe. Other luxury car makers, most notably the Toyota subsidiary Lexus, have enjoyed \n",
            "\n",
            " Response of gpt-3.5-turbo: \n",
            " General Motors has confirmed that it will build a new medium-sized Cadillac BLS at its Saab factory in Sweden. The car, intended for the European market, will be the first Cadillac with a diesel engine. This move is part of GM's efforts to make Cadillac appeal to European drivers. The decision to build Cadillacs in Sweden may impact GM's Opel workers in Germany, but reflects Saab's struggle to compete in the European luxury car market.\n",
            "\n",
            " Response of GPT-4: \n",
            " General Motors (GM) has confirmed plans to build a new medium-sized Cadillac BLS at its Saab factory in Sweden. The car, which will be the first Cadillac with a diesel engine, is intended to compete in the medium-sized luxury car market but will not be sold in the US. The move is part of GM's efforts to make the Cadillac brand appeal to European drivers. The announcement may alleviate fears about the closure of the Saab factory, which has been the subject of rumours due to GM's planned cutbacks in Europe. However, the decision reflects Saab's failure to make significant inroads into the European luxury car market.\n",
            "\n",
            " Response of GPT-4omni: \n",
            " General Motors (GM) has confirmed plans to build a new medium-sized Cadillac BLS at its Saab factory in Trollhättan, Sweden. This move is part of GM's strategy to make the Cadillac brand more appealing to European drivers, and the BLS will be the first Cadillac to feature a diesel engine. The car, unveiled at the Geneva motor show, is intended to compete in the medium-sized luxury car market but will not be sold in the US. This decision helps alleviate concerns about the potential closure of the Saab factory, although it may shift the burden of GM's cutbacks to its Opel workers in Germany. Despite this new commitment, the announcement also highlights Saab's struggles to compete effectively in the European luxury car market against established brands like BMW, Mercedes, and Jaguar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "\n",
        "This query is a bit more generic than previous one although both are related to the same section in the document. I was particularly interested in if the RAG will be able to detect the similarity of the context as the similarity score will be much lower in this case. In this setup with threshold of 0.3, it seems to correctly detect and provide the appropriate response.\n",
        "\n",
        "In regards to the LLMs, they all seem to provide just the response from the document. My query was to tell the user about Cadillac and Sweden. It could have responded with the history of Cadillac and other information; however, none of that is in their response. They all all, including 3.5-turbo, seems to be able to ignore the off-topic responses."
      ],
      "metadata": {
        "id": "NwlKUmk0LcFa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparison with ChatGPT-4o (chatbot)\n",
        "\n",
        "**Below are the responses from the GPT-4o chatbot**\n",
        "\n",
        "Is Taylor Swift better than Katy Perry?\n",
        "\n",
        "The provided text does not contain any information or comparison between Taylor Swift and Katy Perry. Therefore, no conclusion can be drawn from the given text.\n",
        "Is General Motors building new Cadillac BLS at Saab factory in Sweden?\n",
        "\n",
        "Yes, General Motors confirmed that it will build a new medium-sized Cadillac BLS at its loss-making Saab factory in Sweden. This information is stated in the provided text.\n",
        "Tell me about Cadillac and Sweden.\n",
        "\n",
        "General Motors, the world's largest car maker, confirmed that it would build a new medium-sized Cadillac BLS at its Saab factory in Sweden. The Cadillac BLS is intended to compete in the medium-sized luxury car market and will be the first Cadillac with a diesel engine. This decision should help alleviate fears of the Saab factory's closure. The factory in Trollhaettan has been at the center of rumors about GM's planned severe cutbacks in its troubled European operations. Building Cadillac in Sweden is part of GM's efforts to push the brand as an alternative luxury option for European drivers, where it has established itself as an upmarket brand in the US. This move is intended to introduce much-needed scale to the Saab factory, which currently produces fewer than 130,000 cars per year, about half of what is considered sufficient for profitable operations.\n",
        "\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "As seen from the responses above, it is clear that the chatbot is not subject to the vulnerabilities of Red Teaming. It was so satisfying to see the resonse for the 3rd question, since the prompt was tempting for the Chatbot to incline towards its prior data, but it didn't. In the prompt, I explicitly asked to provide response using 'just the information provided', which could have pushed the LLM to not fall back to its training data.\n",
        "\n",
        "This was done with GPT-4o which has been performing well for Red Teaming purporses, however chatgpt-3.5-turbo could behave differently. It wasn't experimented since 3.5-turbo don't accept pdf files.\n"
      ],
      "metadata": {
        "id": "FULiPGoMPCPX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cost of the Assignment\n",
        "During the entire time of working on this assignment, it cost me around 25 cents, which isn't much. However one of my friends reported a meger 10 cents. The higher cost for me could be because I had been running the 'large' embedding model since beginning of the assignment. On top of that, I also used all three GPT models: 3.5-turbo, GPT-4, GPT-4o for the validation purposes which all add more costs over the course of the assignment completion."
      ],
      "metadata": {
        "id": "SSXz_bHpm_LA"
      }
    }
  ]
}